{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78ba22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset as torchDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "143e949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER_PATH = './data'\n",
    "EN_SENTENCES_PATH = DATA_FOLDER_PATH + \"/microlang_20000_eng.txt\"\n",
    "SW_SENTENCES_PATH = DATA_FOLDER_PATH + \"/microlang_20000_swe.txt\"\n",
    "EN_VOCAB_PATH = DATA_FOLDER_PATH + \"/eng_vocab.json\"\n",
    "SW_VOCAB_PATH = DATA_FOLDER_PATH + \"/swe_vocab.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907dcb8",
   "metadata": {},
   "source": [
    "## Load data and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98a10a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_files(en_file_path, sw_file_path):\n",
    "    \n",
    "    with open(en_file_path, 'r', encoding='utf-8') as en_file:\n",
    "        en_sentences = [line.strip() for line in en_file]\n",
    "        \n",
    "    with open(sw_file_path, 'r', encoding='utf-8') as sw_file:\n",
    "        sw_sentences = [line.strip() for line in sw_file]\n",
    "        \n",
    "    dataset = Dataset.from_dict({\"en\": en_sentences, \"sw\": sw_sentences})\n",
    "    return dataset    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb35d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data_from_files(EN_SENTENCES_PATH, SW_SENTENCES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72ee5e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['en', 'sw'],\n",
      "    num_rows: 20000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5588831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English unique words: 161\n",
      "Swedish unique words: 248\n"
     ]
    }
   ],
   "source": [
    "def count_unique_words(dataset, field):\n",
    "    vocab = set()\n",
    "    for sentence in dataset[field]:\n",
    "        for word in sentence.lower().split():\n",
    "            vocab.add(word)\n",
    "    return len(vocab)\n",
    "\n",
    "en_unique = count_unique_words(dataset, \"en\")\n",
    "sw_unique = count_unique_words(dataset, \"sw\")\n",
    "\n",
    "print(\"English unique words:\", en_unique)\n",
    "print(\"Swedish unique words:\", sw_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f55bf",
   "metadata": {},
   "source": [
    "This is not surpising as Swedish is morphologically much more complex than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fa80d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['en', 'sw'],\n",
      "    num_rows: 16000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['en', 'sw'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['en', 'sw'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "temp = dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset = {\n",
    "    \"train\": dataset[\"train\"],\n",
    "    \"val\": temp[\"train\"],\n",
    "    \"test\": temp[\"test\"],\n",
    "}\n",
    "train_data, val_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"val\"],\n",
    "    dataset[\"test\"],\n",
    ")\n",
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b251c1a",
   "metadata": {},
   "source": [
    "## Define tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b40bbe",
   "metadata": {},
   "source": [
    "Define special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1fcb8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = \"<SOS>\"\n",
    "eos_token = \"<EOS>\"\n",
    "unk_token = \"<UNK>\"\n",
    "pad_token = \"<PAD>\"\n",
    "special_tokens = [unk_token, pad_token, sos_token, eos_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ab8c3",
   "metadata": {},
   "source": [
    "Since there are so few unique words in the dataset (161 and 248), there is no need to use any fancy pancy tokenizers. I will build my own lil tokenizer, which will be just fine for this job. Even using spaCy would seem a bit redundant since there is no punctuation and no commas in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06506c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lilTokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {pad_token: 0, sos_token: 1, eos_token: 2, unk_token: 3}\n",
    "        self.idx2word = {0: pad_token, 1: sos_token, 2: eos_token, 3:unk_token}\n",
    "        self.vocab_size = 4\n",
    "        \n",
    "    def build_vocab(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.lower().split():\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = self.vocab_size\n",
    "                    self.idx2word[self.vocab_size] = word\n",
    "                    self.vocab_size += 1\n",
    "                \n",
    "    def encode(self, sentence):\n",
    "        '''Convert a sentence to integers and wrap it in eos and sos tokens'''\n",
    "        tokens = []\n",
    "        for word in sentence.lower().split():\n",
    "            tokens.append(self.word2idx.get(word, self.word2idx[unk_token]))\n",
    "        return [self.word2idx[sos_token]] + tokens + [self.word2idx[eos_token]]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        '''Convers integers back to readable text'''\n",
    "        words = []\n",
    "        for idx in token_ids:\n",
    "            word = self.idx2word[idx] \n",
    "            # self.idx2word.get(idx, unk_token) would off course be better practice,\n",
    "            # but I want to make sure I get an error if something is misaligend in my code.\n",
    "            # So I will leave it like this for learning purposes!\n",
    "            if word == eos_token:\n",
    "                break\n",
    "            if word not in [sos_token, pad_token]:\n",
    "                words.append(word)\n",
    "        return \" \".join(words) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eef0ed81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 16000\n"
     ]
    }
   ],
   "source": [
    "en_sentences_train = [sentence for sentence in train_data[\"en\"]]\n",
    "sw_sentences_train = [sentence for sentence in train_data[\"sw\"]]\n",
    "print(len(en_sentences_train), len(sw_sentences_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc6eaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = lilTokenizer()\n",
    "en_tokenizer.build_vocab(en_sentences_train)\n",
    "sw_tokenizer = lilTokenizer()\n",
    "sw_tokenizer.build_vocab(sw_sentences_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e4c9b",
   "metadata": {},
   "source": [
    "Lil sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a359b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token indices: 0, 1, 2, 3\n"
     ]
    }
   ],
   "source": [
    "assert en_tokenizer.word2idx[pad_token] == sw_tokenizer.word2idx[pad_token]\n",
    "assert en_tokenizer.word2idx[sos_token] == sw_tokenizer.word2idx[sos_token]\n",
    "assert en_tokenizer.word2idx[eos_token] == sw_tokenizer.word2idx[eos_token]\n",
    "assert en_tokenizer.word2idx[unk_token] == sw_tokenizer.word2idx[unk_token]\n",
    "\n",
    "pad_idx = en_tokenizer.word2idx[pad_token]\n",
    "sos_idx = en_tokenizer.word2idx[sos_token]\n",
    "eos_idx = en_tokenizer.word2idx[eos_token]\n",
    "unk_idx = en_tokenizer.word2idx[unk_token]\n",
    "\n",
    "# should be 0, 1, 2, 3\n",
    "print(f\"Special token indices: {pad_idx}, {sos_idx}, {eos_idx}, {unk_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d15c130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 252\n"
     ]
    }
   ],
   "source": [
    "# should be 161 + 4 = 165 and 248 + 4 = 252\n",
    "print(len(en_tokenizer.word2idx), len(sw_tokenizer.word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4755a3",
   "metadata": {},
   "source": [
    "### Save vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9fc564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(tokenizer, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tokenizer.word2idx, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ebeed24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(en_tokenizer, EN_VOCAB_PATH)\n",
    "save_vocab(sw_tokenizer, SW_VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda122b",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc4bfe",
   "metadata": {},
   "source": [
    "Alright, lets get these sentences into pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83708ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(torchDataset):\n",
    "    def __init__(self, en_sentences, sw_sentences, en_tokenizer, sw_tokenizer):\n",
    "        self.en_sentences = en_sentences\n",
    "        self.sw_sentences = sw_sentences\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.sw_tokenizer = sw_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # raw text for a specific index\n",
    "        en_text = self.en_sentences[idx]\n",
    "        sw_text = self.sw_sentences[idx]\n",
    "\n",
    "        # encode texts using my awesome lilTokenizer\n",
    "        en_encoded = self.en_tokenizer.encode(en_text)\n",
    "        sw_encoded = self.sw_tokenizer.encode(sw_text)\n",
    "\n",
    "        # convert the integer lists into pytorch tensors!\n",
    "        return torch.tensor(en_encoded), torch.tensor(sw_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "23e6bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    en_batch = []\n",
    "    sw_batch = []\n",
    "    \n",
    "    # separate tensors\n",
    "    for en_item, sw_item in batch:\n",
    "        en_batch.append(en_item)\n",
    "        sw_batch.append(sw_item)\n",
    "        \n",
    "    # pad the sequences\n",
    "    # batch_first=False creates shape: (Sequence_Length, Batch_Size)\n",
    "    en_padded = pad_sequence(en_batch, padding_value=pad_idx, batch_first=False)\n",
    "    sw_padded = pad_sequence(sw_batch, padding_value=pad_idx, batch_first=False)\n",
    "    \n",
    "    return en_padded, sw_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b442db55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Batch Shape: torch.Size([8, 32]) (Seq_Len, Batch_Size)\n",
      "Swedish Batch Shape: torch.Size([8, 32]) (Seq_Len, Batch_Size)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TranslationDataset(\n",
    "    en_sentences=en_sentences_train, \n",
    "    sw_sentences=sw_sentences_train, \n",
    "    en_tokenizer=en_tokenizer, \n",
    "    sw_tokenizer=sw_tokenizer\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# check first batch\n",
    "en_batch, sw_batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"English Batch Shape: {en_batch.shape} (Seq_Len, Batch_Size)\")\n",
    "print(f\"Swedish Batch Shape: {sw_batch.shape} (Seq_Len, Batch_Size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c55b3",
   "metadata": {},
   "source": [
    "## Define Encoder and Decoder for Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10159b",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b482cd",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
