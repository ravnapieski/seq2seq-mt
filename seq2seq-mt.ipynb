{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2db862",
   "metadata": {},
   "source": [
    "# SEQ2SEQ MT (English-Swedish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "78ba22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset as torchDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "143e949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER_PATH = './data'\n",
    "EN_SENTENCES_PATH = DATA_FOLDER_PATH + \"/microlang_20000_eng.txt\"\n",
    "SW_SENTENCES_PATH = DATA_FOLDER_PATH + \"/microlang_20000_swe.txt\"\n",
    "EN_VOCAB_PATH = DATA_FOLDER_PATH + \"/eng_vocab.json\"\n",
    "SW_VOCAB_PATH = DATA_FOLDER_PATH + \"/swe_vocab.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2e22951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "DROPOUT = 0.3\n",
    "LR = 0.001\n",
    "TF_RATIO = 0.5\n",
    "CLIP_GRAD = 1.0\n",
    "\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907dcb8",
   "metadata": {},
   "source": [
    "## Load data and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98a10a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_files(en_file_path, sw_file_path):\n",
    "    \n",
    "    with open(en_file_path, 'r', encoding='utf-8') as en_file:\n",
    "        en_sentences = [line.strip() for line in en_file]\n",
    "        \n",
    "    with open(sw_file_path, 'r', encoding='utf-8') as sw_file:\n",
    "        sw_sentences = [line.strip() for line in sw_file]\n",
    "        \n",
    "    dataset = Dataset.from_dict({\"en\": en_sentences, \"sw\": sw_sentences})\n",
    "    return dataset    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fb35d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data_from_files(EN_SENTENCES_PATH, SW_SENTENCES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "72ee5e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['en', 'sw'],\n",
      "    num_rows: 20000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5588831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English unique words: 161\n",
      "Swedish unique words: 248\n"
     ]
    }
   ],
   "source": [
    "def count_unique_words(dataset, field):\n",
    "    vocab = set()\n",
    "    for sentence in dataset[field]:\n",
    "        for word in sentence.lower().split():\n",
    "            vocab.add(word)\n",
    "    return len(vocab)\n",
    "\n",
    "en_unique = count_unique_words(dataset, \"en\")\n",
    "sw_unique = count_unique_words(dataset, \"sw\")\n",
    "\n",
    "print(\"English unique words:\", en_unique)\n",
    "print(\"Swedish unique words:\", sw_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f55bf",
   "metadata": {},
   "source": [
    "This is not surpising as Swedish is morphologically much more complex than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8fa80d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['en', 'sw'],\n",
      "    num_rows: 16000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['en', 'sw'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['en', 'sw'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "temp = dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "dataset = {\n",
    "    \"train\": dataset[\"train\"],\n",
    "    \"val\": temp[\"train\"],\n",
    "    \"test\": temp[\"test\"],\n",
    "}\n",
    "train_data, val_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"val\"],\n",
    "    dataset[\"test\"],\n",
    ")\n",
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b251c1a",
   "metadata": {},
   "source": [
    "## Define tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b40bbe",
   "metadata": {},
   "source": [
    "Define special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d1fcb8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = \"<SOS>\"\n",
    "eos_token = \"<EOS>\"\n",
    "unk_token = \"<UNK>\"\n",
    "pad_token = \"<PAD>\"\n",
    "special_tokens = [unk_token, pad_token, sos_token, eos_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ab8c3",
   "metadata": {},
   "source": [
    "Since there are so few unique words in the dataset (161 and 248), there is no need to use any fancy pancy tokenizers. I will build my own lil tokenizer, which will be just fine for this job. Even using spaCy would seem a bit redundant since there is no punctuation and no commas in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "06506c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class lilTokenizer:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {pad_token: 0, sos_token: 1, eos_token: 2, unk_token: 3}\n",
    "        self.idx2word = {0: pad_token, 1: sos_token, 2: eos_token, 3:unk_token}\n",
    "        self.vocab_size = 4\n",
    "        \n",
    "    def build_vocab(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            for word in sentence.lower().split():\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = self.vocab_size\n",
    "                    self.idx2word[self.vocab_size] = word\n",
    "                    self.vocab_size += 1\n",
    "                \n",
    "    def encode(self, sentence):\n",
    "        '''Convert a sentence to integers and wrap it in eos and sos tokens'''\n",
    "        tokens = []\n",
    "        for word in sentence.lower().split():\n",
    "            tokens.append(self.word2idx.get(word, self.word2idx[unk_token]))\n",
    "        return [self.word2idx[sos_token]] + tokens + [self.word2idx[eos_token]]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        '''Convers integers back to readable text'''\n",
    "        words = []\n",
    "        for idx in token_ids:\n",
    "            word = self.idx2word[idx] \n",
    "            # self.idx2word.get(idx, unk_token) would off course be better practice,\n",
    "            # but I want to make sure I get an error if something is misaligend in my code.\n",
    "            # So I will leave it like this for learning purposes!\n",
    "            if word == eos_token:\n",
    "                break\n",
    "            if word not in [sos_token, pad_token]:\n",
    "                words.append(word)\n",
    "        return \" \".join(words) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eef0ed81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 16000\n",
      "2000 2000\n",
      "2000 2000\n"
     ]
    }
   ],
   "source": [
    "en_sentences_train = [sentence for sentence in train_data[\"en\"]]\n",
    "sw_sentences_train = [sentence for sentence in train_data[\"sw\"]]\n",
    "print(len(en_sentences_train), len(sw_sentences_train))\n",
    "\n",
    "en_sentences_val = [sentence for sentence in val_data[\"en\"]]\n",
    "sw_sentences_val = [sentence for sentence in val_data[\"sw\"]]\n",
    "print(len(en_sentences_val), len(sw_sentences_val))\n",
    "\n",
    "en_sentences_test = [sentence for sentence in test_data[\"en\"]]\n",
    "sw_sentences_test = [sentence for sentence in test_data[\"sw\"]]\n",
    "print(len(en_sentences_test), len(sw_sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc6eaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = lilTokenizer()\n",
    "en_tokenizer.build_vocab(en_sentences_train)\n",
    "sw_tokenizer = lilTokenizer()\n",
    "sw_tokenizer.build_vocab(sw_sentences_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e4c9b",
   "metadata": {},
   "source": [
    "Lil sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a359b487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token indices: 0, 1, 2, 3\n"
     ]
    }
   ],
   "source": [
    "assert en_tokenizer.word2idx[pad_token] == sw_tokenizer.word2idx[pad_token]\n",
    "assert en_tokenizer.word2idx[sos_token] == sw_tokenizer.word2idx[sos_token]\n",
    "assert en_tokenizer.word2idx[eos_token] == sw_tokenizer.word2idx[eos_token]\n",
    "assert en_tokenizer.word2idx[unk_token] == sw_tokenizer.word2idx[unk_token]\n",
    "\n",
    "PAD_IDX = en_tokenizer.word2idx[pad_token]\n",
    "SOS_IDX = en_tokenizer.word2idx[sos_token]\n",
    "EOS_IDX = en_tokenizer.word2idx[eos_token]\n",
    "UNK_IDX = en_tokenizer.word2idx[unk_token]\n",
    "\n",
    "# should be 0, 1, 2, 3\n",
    "print(f\"Special token indices: {PAD_IDX}, {SOS_IDX}, {EOS_IDX}, {UNK_IDX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3d15c130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 252\n"
     ]
    }
   ],
   "source": [
    "# should be 161 + 4 = 165 and 248 + 4 = 252\n",
    "print(len(en_tokenizer.word2idx), len(sw_tokenizer.word2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4755a3",
   "metadata": {},
   "source": [
    "### Save vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a9fc564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocab(tokenizer, filepath):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tokenizer.word2idx, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ebeed24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_vocab(en_tokenizer, EN_VOCAB_PATH)\n",
    "save_vocab(sw_tokenizer, SW_VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda122b",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc4bfe",
   "metadata": {},
   "source": [
    "Alright, lets get these sentences into pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "83708ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(torchDataset):\n",
    "    def __init__(self, en_sentences, sw_sentences, en_tokenizer, sw_tokenizer):\n",
    "        self.en_sentences = en_sentences\n",
    "        self.sw_sentences = sw_sentences\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.sw_tokenizer = sw_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.en_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # raw text for a specific index\n",
    "        en_text = self.en_sentences[idx]\n",
    "        sw_text = self.sw_sentences[idx]\n",
    "\n",
    "        # encode texts using my awesome lilTokenizer\n",
    "        en_encoded = self.en_tokenizer.encode(en_text)\n",
    "        sw_encoded = self.sw_tokenizer.encode(sw_text)\n",
    "\n",
    "        # convert the integer lists into pytorch tensors!\n",
    "        return torch.tensor(en_encoded), torch.tensor(sw_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e6bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    en_batch = []\n",
    "    sw_batch = []\n",
    "    \n",
    "    # separate tensors\n",
    "    for en_item, sw_item in batch:\n",
    "        en_batch.append(en_item)\n",
    "        sw_batch.append(sw_item)\n",
    "        \n",
    "    # pad the sequences\n",
    "    # batch_first=False creates shape: (Sequence_Length, Batch_Size)\n",
    "    en_padded = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    sw_padded = pad_sequence(sw_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "    \n",
    "    return en_padded, sw_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b442db55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Batch Shape: torch.Size([8, 32]) (Seq_Len, Batch_Size)\n",
      "Swedish Batch Shape: torch.Size([8, 32]) (Seq_Len, Batch_Size)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TranslationDataset(\n",
    "    en_sentences=en_sentences_train, \n",
    "    sw_sentences=sw_sentences_train, \n",
    "    en_tokenizer=en_tokenizer, \n",
    "    sw_tokenizer=sw_tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = TranslationDataset(\n",
    "    en_sentences=en_sentences_val,\n",
    "    sw_sentences=sw_sentences_val,\n",
    "    en_tokenizer=en_tokenizer,\n",
    "    sw_tokenizer=sw_tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = TranslationDataset(\n",
    "    en_sentences=en_sentences_test,\n",
    "    sw_sentences=sw_sentences_test,\n",
    "    en_tokenizer=en_tokenizer,\n",
    "    sw_tokenizer=sw_tokenizer\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# check first batch\n",
    "en_batch, sw_batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"English Batch Shape: {en_batch.shape} (Seq_Len, Batch_Size)\")\n",
    "print(f\"Swedish Batch Shape: {sw_batch.shape} (Seq_Len, Batch_Size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c55b3",
   "metadata": {},
   "source": [
    "## Define Encoder and Decoder for Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7afda",
   "metadata": {},
   "source": [
    "I choose... GRU! \n",
    "\n",
    "Because, they are a type of RNN that efficiently capture sequential dependencies while having fewer parameters than LSTMs. This makes them faster to train and less prone to overfitting on a small dataset like this, while still handling long-range dependencies better than a vanilla RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10159b",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d01b01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, dropout=0.3):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # pass padding_idx so the model ignores it and can focus on learning\n",
    "        # the actual words!\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (src_len, batch_size)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b482cd",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6521f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx, dropout=0.3):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # x: (batch_size)\n",
    "        x = x.unsqueeze(0) # (1, batch_size)\n",
    "        embedded = self.dropout(self.embedding(x)) # (1, batch_size, embed_dim)\n",
    "        outputs, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc_out(outputs) # (batch_size, vocab_size)\n",
    "        return prediction.squeeze(0), hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7e963",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "67aa2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        # ensure the Encoder and Decoder hidden sizes match\n",
    "        assert encoder.rnn.hidden_size == decoder.rnn.hidden_size, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!!!!\"\n",
    "\n",
    "    def forward(self, source, target, tf_ratio=0.5):\n",
    "        # source shape: (src_len, batch_size)\n",
    "        # target shape: (trg_len, batch_size)\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder.fc_out.out_features\n",
    "        \n",
    "        # tensor to store all decoder predictions\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "        \n",
    "        # pass sentence through the encoder \n",
    "        hidden = self.encoder(source) # this is the so called context vector\n",
    "        \n",
    "        # first input to the decoder is the [SOS] tokens from the target\n",
    "        input_word = target[0, :]\n",
    "        \n",
    "        # generate word by word\n",
    "        for t in range(1, target_len):\n",
    "            # pass current word and previous hidden state into decoder\n",
    "            prediction, hidden = self.decoder(input_word, hidden)\n",
    "            \n",
    "            outputs[t] = prediction\n",
    "            \n",
    "            teacher_force = random.random() < tf_ratio\n",
    "            \n",
    "            top1 = prediction.argmax(1) # models best guess\n",
    "            \n",
    "            # ground truth or model's own guess\n",
    "            input_word = target[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "54945978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size=en_tokenizer.vocab_size, embed_dim=EMBED_DIM,\n",
    "                  hidden_dim=HIDDEN_DIM, pad_idx=PAD_IDX, dropout=DROPOUT)\n",
    "decoder = Decoder(vocab_size=sw_tokenizer.vocab_size, embed_dim=EMBED_DIM,\n",
    "                  hidden_dim=HIDDEN_DIM, pad_idx=PAD_IDX, dropout=DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8a41f",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7891b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip_grad=CLIP_GRAD, tf_radio=TF_RATIO):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        output = model(src, trg, tf_ratio=tf_radio)\n",
    "        \n",
    "        # output shape is currently: (trg_len, batch_size, output_dim)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # slice off the 0th token [SOS] and flatten the rest\n",
    "        output = output[1:].view(-1, output_dim) \n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eba1eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # turn of gradient tracking\n",
    "    with torch.no_grad():\n",
    "        for i, (src, trg) in enumerate(iterator):\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            \n",
    "            # forward pass (no teacher forcing)\n",
    "            output = model(src, trg, tf_ratio=0)\n",
    "            \n",
    "            # flatten\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5712209e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:09<01:28,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epch: 1\n",
      "Epoch: 01\n",
      "\t train Loss: 1.984 | train perplexity:   7.274\n",
      "\t val Loss: 0.988 |  val perplexity:   2.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:20<01:21, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epch: 2\n",
      "Epoch: 02\n",
      "\t train Loss: 0.693 | train perplexity:   2.000\n",
      "\t val Loss: 0.444 |  val perplexity:   1.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:29<01:09,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epch: 3\n",
      "Epoch: 03\n",
      "\t train Loss: 0.281 | train perplexity:   1.324\n",
      "\t val Loss: 0.134 |  val perplexity:   1.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:39<00:58,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epch: 4\n",
      "Epoch: 04\n",
      "\t train Loss: 0.090 | train perplexity:   1.094\n",
      "\t val Loss: 0.055 |  val perplexity:   1.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:49<00:49,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epch: 5\n",
      "Epoch: 05\n",
      "\t train Loss: 0.039 | train perplexity:   1.039\n",
      "\t val Loss: 0.032 |  val perplexity:   1.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:59<00:40, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epch: 6\n",
      "Epoch: 06\n",
      "\t train Loss: 0.021 | train perplexity:   1.021\n",
      "\t val Loss: 0.018 |  val perplexity:   1.018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:09<00:29,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epch: 7\n",
      "Epoch: 07\n",
      "\t train Loss: 0.012 | train perplexity:   1.012\n",
      "\t val Loss: 0.012 |  val perplexity:   1.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:18<00:19,  9.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epch: 8\n",
      "Epoch: 08\n",
      "\t train Loss: 0.009 | train perplexity:   1.009\n",
      "\t val Loss: 0.009 |  val perplexity:   1.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:29<00:10, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09\n",
      "\t train Loss: 0.011 | train perplexity:   1.011\n",
      "\t val Loss: 0.016 |  val perplexity:   1.016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:40<00:00, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epch: 10\n",
      "Epoch: 10\n",
      "\t train Loss: 0.007 | train perplexity:   1.007\n",
      "\t val Loss: 0.007 |  val perplexity:   1.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf') # set starting best loss to infinity\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    \n",
    "    # train the model and get training loss\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    \n",
    "    # evaluate the model and get validation loss\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    # save the model if its the best one yet!!\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        print(f\"Saving model at epch: {epoch+1}\")\n",
    "        torch.save(model.state_dict(), 'best_translation_model.pt')\n",
    "    \n",
    "    # print stats\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\t train Loss: {train_loss:.3f} | train perplexity: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t val Loss: {valid_loss:.3f} |  val perplexity: {math.exp(valid_loss):7.3f}')\n",
    "    # A lower perplexity means the model is more confident in its translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743cb1d",
   "metadata": {},
   "source": [
    "Pretty nice! A perplexity of 1.0 means the model has 0% confusion. So when the model hits a perplexity of 1.007, it means it is basically 100% certain of the correct Swedish word at every single step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466c1a3",
   "metadata": {},
   "source": [
    "And even though the dataset was very small, the model was able to practically memorize the grammar and vocab perfectly without overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaddb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_translation_model.pt\"))\n",
    "\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6b3e8270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max English length: 10\n",
      "Max Swedish length: 10\n"
     ]
    }
   ],
   "source": [
    "def get_max_len(sentences_list, tokenizer):\n",
    "    # sentences_list: list of lists of sentences\n",
    "    max_len = 0\n",
    "    for sentences in sentences_list:\n",
    "        for s in sentences:\n",
    "            length = len(tokenizer.encode(s))\n",
    "            if length > max_len:\n",
    "                max_len = length\n",
    "    return max_len\n",
    "\n",
    "all_en = [en_sentences_train, en_sentences_val, en_sentences_test]\n",
    "all_sw = [sw_sentences_train, sw_sentences_val, sw_sentences_test]\n",
    "\n",
    "max_len_en = get_max_len(all_en, en_tokenizer)\n",
    "max_len_sw = get_max_len(all_sw, sw_tokenizer)\n",
    "\n",
    "print(f\"Max English length: {max_len_en}\")\n",
    "print(f\"Max Swedish length: {max_len_sw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a02094c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, en_tokenizer, sw_tokenizer, device, max_len=15):\n",
    "    model.eval()\n",
    "    \n",
    "    # encode the English sentence\n",
    "    tokens = en_tokenizer.encode(sentence)\n",
    "    \n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(1).to(device)\n",
    "    \n",
    "    # get the context vector from the Encoder\n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(src_tensor)\n",
    "        \n",
    "    # init the target sequence with the [SOS] token ID\n",
    "    trg_indexes = [SOS_IDX]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # the input to the decoder is  the \"last\" word predicted\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden = model.decoder(trg_tensor, hidden)\n",
    "            \n",
    "        # output shape is (1, swedish_vocab_size). argmax(1) gets the highest probability index\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        # stop early if the model predicts the end of the sentence\n",
    "        if pred_token == EOS_IDX:\n",
    "            break\n",
    "            \n",
    "    # convert the list of predicted integers back to a readable string\n",
    "    translated_sentence = sw_tokenizer.decode(trg_indexes)\n",
    "    \n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f88a1",
   "metadata": {},
   "source": [
    "lil sanity check again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dd471069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: a red apple is hot\n",
      "Swedish: ett rött äpple är hett\n"
     ]
    }
   ],
   "source": [
    "# Try a sentence from your dataset\n",
    "english_sentence = \"a red apple is hot\"\n",
    "swedish_translation = translate_sentence(english_sentence, model, en_tokenizer, sw_tokenizer, device)\n",
    "\n",
    "print(f\"English: {english_sentence}\")\n",
    "print(f\"Swedish: {swedish_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b1950",
   "metadata": {},
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4291b6a",
   "metadata": {},
   "source": [
    "## BLEU score calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b80432",
   "metadata": {},
   "source": [
    "pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2f42a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sacrebleu(test_pairs, model, en_tokenizer, sw_tokenizer, device):\n",
    "    \"\"\"\n",
    "    test_pairs should be a list of tuples containing the raw strings: \n",
    "    [(\"a red apple is hot\", \"ett rött äpple är hett\"), ...]\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    print(\"Translating test set...\")\n",
    "    # loop through all sentence pairs in test set\n",
    "    for item in tqdm(test_pairs):\n",
    "        eng_sentence = item[\"en\"]\n",
    "        true_sw_sentence = item[\"sw\"]\n",
    "        \n",
    "        # translate english sentence\n",
    "        predicted_sw = translate_sentence(eng_sentence, model, en_tokenizer, sw_tokenizer, device)\n",
    "        predictions.append(predicted_sw)\n",
    "        references.append(true_sw_sentence)\n",
    "        \n",
    "    # format references for SacreBLEU (list of lists)\n",
    "    references_formatted = [references]\n",
    "    \n",
    "    # calculate the BLEU score !!!\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, references_formatted)\n",
    "    \n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5a167a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:08<00:00, 222.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** FINAL RESULTS *****\n",
      "SacreBLEU Score: 99.56\n",
      "All details: BLEU = 99.56 99.8/99.7/99.5/99.3 (BP = 1.000 ratio = 1.000 hyp_len = 7285 ref_len = 7285)\n"
     ]
    }
   ],
   "source": [
    "bleu_result = calculate_sacrebleu(test_data, model, en_tokenizer, sw_tokenizer, device)\n",
    "\n",
    "print(f\"\\n***** FINAL RESULTS *****\")\n",
    "print(f\"SacreBLEU Score: {bleu_result.score:.2f}\")\n",
    "print(f\"All details: {bleu_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b6bb9",
   "metadata": {},
   "source": [
    "Scores over 0.60 are often considered better than human so a score of 99.56 is obviously very very very good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930fd096",
   "metadata": {},
   "source": [
    "But... we are using a \"microlang\" dataset with a tiny vocabulary and highly predictable, punctuation-free grammar. So the GRU model had enough capacity to practically memorize the exact mapping rules of this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
